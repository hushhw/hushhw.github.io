---
title: 吴恩达《机器学习》笔记（五）——正则化
comments: true
mathjax: true
toc: true
tocnumber: true
music: false
image: false
abbrlink: 76f89b87
date: 2019-08-01 16:54:09
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
---



## Regularization

### 7.1. Overfitting

机器学习训练的目的是为了让模型更好的拟合实际情况，从而指导我们进行预测。评价一个模型拟合度是否优良的参考之一是它与实际数据集的偏差程度，我们用代价函数来定量，一般代价函数越小越好，但是当将它们应用在某些特定的机器学习应用时，会遇到**过拟合（over-fitting）**的问题。

下图是一个回归问题的例子：

第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。 

![](https://photo.hushhw.cn/20190805144528.png)

下图是一个分类问题的例子：

以多项式理解，𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 

![](https://photo.hushhw.cn/20190805144934.png)

由上述分析，我们可以由两种解决过拟合的方案：

* 丢弃一些不能帮助我们正确预测的特征：可以手动选择保留哪些特征，或者使用一些模型选择的算法来帮忙
* 正则化：保留所有的特征变量，但是减少量级或参数 $\theta_j$ 的大小



​            

### 7.2. Cost Function

为了避免过拟合的问题，这里介绍前面提到的正则化的方法。

上面的回归问题中我们知道，正是那些高次项导致了过拟合的问题，所以如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。

所以，这里我们通过降低三次项和四次项的权重，即令 $𝜃_3$ 和 $𝜃_4$ 的值很小，使得拟合模型在大方向上是「二次多项式」在发挥作用。我们要做的便是修改代价函数，在其中 $𝜃_3$ 和 $𝜃_4$ 设置一点惩罚（penalize）。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 $𝜃_3$ 和 $𝜃_4​$ 。 

![](https://photo.hushhw.cn/20190805152238.png)

通过这样的代价函数选择出的 $𝜃_3$ 和 $𝜃_4$ 对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：
$$
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]
$$
其中 𝜆 又称为**正则化参数（Regularization Parameter）**。 根据惯例，我们不对 $𝜃_0​$ 进行惩罚。

对 𝜆 的取值要合理，如果 𝜆 过大的话，为了使得代价函数尽可能小，所有的 𝜃 值（不包括 $𝜃_0​$）就都会在一定程度上减小，我们只能得到一条平行于 𝑥 轴的直线，这样会导致**欠拟合（underfitting)**。

​                 

### 7.3. Regularized Linear Regression

对于线性回归的求解，我们之前推到了两种学习算法：

* 梯度下降
* 正规方程

正则化线性回归的代价函数为：
$$
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]
$$
如果我们要使用梯度下降法令这个代价函数最小化，其正则化迭代式如下：
$$
\begin{align}
Repeat \{ \\
θ_0 &:= θ_0-\alpha\frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_0 \\
θ_j &:= θ_j-\alpha \left[ \left( \frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}θ_j \right] \ j\in\{1,2,\cdots,n\}\\
\}
\end{align}
$$
将 $θ_0$ 单独分开的原因上面提到了，其对应的特征量是 1，约定俗成不必正则化。上式可在整理为：
$$
θ_j := θ_j(1-\alpha\frac{\lambda}{m})-\alpha\frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j
$$
在这个形式下，我们可以理解得到，每一次迭代都会多乘一个小于一的系数 $1–α\fracλm$，进而缩小。

​              

如果我们要使用正规方程来求解正则化线性回归模型，方法如下：

![](https://photo.hushhw.cn/20190805165041.png)
$$
\theta = (X^TX + \lambda L)^{-1}X^Ty  \\
where \ L = \left[
\begin{matrix}
0 \qquad\qquad\qquad\qquad \\
\qquad 1  \qquad\qquad\qquad\\
\qquad\qquad1 \qquad\qquad \\
\qquad\qquad\qquad\ddots \qquad \\
\qquad\qquad\qquad\qquad 1
\end{matrix}
\right]
$$
 	其中 L 为(n+1)*(n+1)。

​              

## 专业名词整理

* `overfitting`：过拟合、`underfitting`：欠拟合