---
title: å´æ©è¾¾ã€Šæœºå™¨å­¦ä¹ ã€‹ç¬”è®°ï¼ˆäº”ï¼‰â€”â€”æ­£åˆ™åŒ–
comments: true
mathjax: true
toc: true
tocnumber: true
music: false
image: false
abbrlink: 76f89b87
date: 2019-08-01 16:54:09
tags:
  - Machine Learning
categories:
  - ç¬”è®°æ•´ç†
  - æœºå™¨å­¦ä¹ 
description: 'è¯„ä»·ä¸€ä¸ªæ¨¡å‹æ‹Ÿåˆåº¦æ˜¯å¦ä¼˜è‰¯çš„å‚è€ƒä¹‹ä¸€æ˜¯å®ƒä¸å®é™…æ•°æ®é›†çš„åå·®ç¨‹åº¦ï¼Œæˆ‘ä»¬ç”¨ä»£ä»·å‡½æ•°æ¥å®šé‡ï¼Œä¸€èˆ¬ä»£ä»·å‡½æ•°è¶Šå°è¶Šå¥½ï¼Œä½†æ˜¯å½“å°†å®ƒä»¬åº”ç”¨åœ¨æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œä¼šé‡åˆ°è¿‡æ‹Ÿåˆï¼ˆover-fittingï¼‰çš„é—®é¢˜ã€‚è¿™æ—¶å°±éœ€è¦é‡‡å–ä¸€äº›æªæ–½ï¼Œæ­£åˆ™åŒ–å°±æ˜¯å…¶ä¸­ä¸€ç§æ–¹å¼ã€‚'
---

> è¯„ä»·ä¸€ä¸ªæ¨¡å‹æ‹Ÿåˆåº¦æ˜¯å¦ä¼˜è‰¯çš„å‚è€ƒä¹‹ä¸€æ˜¯å®ƒä¸å®é™…æ•°æ®é›†çš„åå·®ç¨‹åº¦ï¼Œæˆ‘ä»¬ç”¨ä»£ä»·å‡½æ•°æ¥å®šé‡ï¼Œä¸€èˆ¬ä»£ä»·å‡½æ•°è¶Šå°è¶Šå¥½ï¼Œä½†æ˜¯å½“å°†å®ƒä»¬åº”ç”¨åœ¨æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œä¼šé‡åˆ°**è¿‡æ‹Ÿåˆï¼ˆover-fittingï¼‰**çš„é—®é¢˜ã€‚è¿™æ—¶å°±éœ€è¦é‡‡å–ä¸€äº›æªæ–½ï¼Œæ­£åˆ™åŒ–å°±æ˜¯å…¶ä¸­ä¸€ç§æ–¹å¼ã€‚



## Regularization

### 7.1. Overfitting

æœºå™¨å­¦ä¹ è®­ç»ƒçš„ç›®çš„æ˜¯ä¸ºäº†è®©æ¨¡å‹æ›´å¥½çš„æ‹Ÿåˆå®é™…æƒ…å†µï¼Œä»è€ŒæŒ‡å¯¼æˆ‘ä»¬è¿›è¡Œé¢„æµ‹ã€‚è¯„ä»·ä¸€ä¸ªæ¨¡å‹æ‹Ÿåˆåº¦æ˜¯å¦ä¼˜è‰¯çš„å‚è€ƒä¹‹ä¸€æ˜¯å®ƒä¸å®é™…æ•°æ®é›†çš„åå·®ç¨‹åº¦ï¼Œæˆ‘ä»¬ç”¨ä»£ä»·å‡½æ•°æ¥å®šé‡ï¼Œä¸€èˆ¬ä»£ä»·å‡½æ•°è¶Šå°è¶Šå¥½ï¼Œä½†æ˜¯å½“å°†å®ƒä»¬åº”ç”¨åœ¨æŸäº›ç‰¹å®šçš„æœºå™¨å­¦ä¹ åº”ç”¨æ—¶ï¼Œä¼šé‡åˆ°**è¿‡æ‹Ÿåˆï¼ˆover-fittingï¼‰**çš„é—®é¢˜ã€‚

ä¸‹å›¾æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜çš„ä¾‹å­ï¼š

ç¬¬ä¸€ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œæ¬ æ‹Ÿåˆï¼Œä¸èƒ½å¾ˆå¥½åœ°é€‚åº”æˆ‘ä»¬çš„è®­ç»ƒé›†ï¼›ç¬¬ä¸‰ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªå››æ¬¡æ–¹çš„æ¨¡å‹ï¼Œè¿‡äºå¼ºè°ƒæ‹ŸåˆåŸå§‹æ•°æ®ï¼Œè€Œä¸¢å¤±äº†ç®—æ³•çš„æœ¬è´¨ï¼šé¢„æµ‹æ–°æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œè‹¥ç»™å‡ºä¸€ä¸ªæ–°çš„å€¼ä½¿ä¹‹é¢„æµ‹ï¼Œå®ƒå°†è¡¨ç°çš„å¾ˆå·®ï¼Œæ˜¯è¿‡æ‹Ÿåˆï¼Œè™½ç„¶èƒ½éå¸¸å¥½åœ°é€‚åº”æˆ‘ä»¬çš„è®­ç»ƒé›†ä½†åœ¨æ–°è¾“å…¥å˜é‡è¿›è¡Œé¢„æµ‹æ—¶å¯èƒ½ä¼šæ•ˆæœä¸å¥½ï¼›è€Œä¸­é—´çš„æ¨¡å‹ä¼¼ä¹æœ€åˆé€‚ã€‚ 

![](https://photo.hushhw.cn/20190805144528.png)

ä¸‹å›¾æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜çš„ä¾‹å­ï¼š

ä»¥å¤šé¡¹å¼ç†è§£ï¼Œğ‘¥ çš„æ¬¡æ•°è¶Šé«˜ï¼Œæ‹Ÿåˆçš„è¶Šå¥½ï¼Œä½†ç›¸åº”çš„é¢„æµ‹çš„èƒ½åŠ›å°±å¯èƒ½å˜å·®ã€‚ 

![](https://photo.hushhw.cn/20190805144934.png)

ç”±ä¸Šè¿°åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥ç”±ä¸¤ç§è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ¡ˆï¼š

* ä¸¢å¼ƒä¸€äº›ä¸èƒ½å¸®åŠ©æˆ‘ä»¬æ­£ç¡®é¢„æµ‹çš„ç‰¹å¾ï¼šå¯ä»¥æ‰‹åŠ¨é€‰æ‹©ä¿ç•™å“ªäº›ç‰¹å¾ï¼Œæˆ–è€…ä½¿ç”¨ä¸€äº›æ¨¡å‹é€‰æ‹©çš„ç®—æ³•æ¥å¸®å¿™
* æ­£åˆ™åŒ–ï¼šä¿ç•™æ‰€æœ‰çš„ç‰¹å¾å˜é‡ï¼Œä½†æ˜¯å‡å°‘é‡çº§æˆ–å‚æ•° $\theta_j$ çš„å¤§å°



â€‹            

### 7.2. Cost Function

ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œè¿™é‡Œä»‹ç»å‰é¢æåˆ°çš„æ­£åˆ™åŒ–çš„æ–¹æ³•ã€‚

ä¸Šé¢çš„å›å½’é—®é¢˜ä¸­æˆ‘ä»¬çŸ¥é“ï¼Œæ­£æ˜¯é‚£äº›é«˜æ¬¡é¡¹å¯¼è‡´äº†è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬èƒ½è®©è¿™äº›é«˜æ¬¡é¡¹çš„ç³»æ•°æ¥è¿‘äº 0 çš„è¯ï¼Œæˆ‘ä»¬å°±èƒ½å¾ˆå¥½çš„æ‹Ÿåˆäº†ã€‚

æ‰€ä»¥ï¼Œè¿™é‡Œæˆ‘ä»¬é€šè¿‡é™ä½ä¸‰æ¬¡é¡¹å’Œå››æ¬¡é¡¹çš„æƒé‡ï¼Œå³ä»¤ $ğœƒ_3$ å’Œ $ğœƒ_4$ çš„å€¼å¾ˆå°ï¼Œä½¿å¾—æ‹Ÿåˆæ¨¡å‹åœ¨å¤§æ–¹å‘ä¸Šæ˜¯ã€ŒäºŒæ¬¡å¤šé¡¹å¼ã€åœ¨å‘æŒ¥ä½œç”¨ã€‚æˆ‘ä»¬è¦åšçš„ä¾¿æ˜¯ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œåœ¨å…¶ä¸­ $ğœƒ_3$ å’Œ $ğœƒ_4$ è®¾ç½®ä¸€ç‚¹æƒ©ç½šï¼ˆpenalizeï¼‰ã€‚è¿™æ ·åšçš„è¯ï¼Œæˆ‘ä»¬åœ¨å°è¯•æœ€å°åŒ–ä»£ä»·æ—¶ä¹Ÿéœ€è¦å°†è¿™ä¸ªæƒ©ç½šçº³å…¥è€ƒè™‘ä¸­ï¼Œå¹¶æœ€ç»ˆå¯¼è‡´é€‰æ‹©è¾ƒå°ä¸€äº›çš„ $ğœƒ_3$ å’Œ $ğœƒ_4$ ã€‚ 

![](https://photo.hushhw.cn/20190805152238.png)

é€šè¿‡è¿™æ ·çš„ä»£ä»·å‡½æ•°é€‰æ‹©å‡ºçš„ $ğœƒ_3$ å’Œ $ğœƒ_4$ å¯¹é¢„æµ‹ç»“æœçš„å½±å“å°±æ¯”ä¹‹å‰è¦å°è®¸å¤šã€‚å‡å¦‚æˆ‘ä»¬æœ‰éå¸¸å¤šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“å…¶ä¸­å“ªäº›ç‰¹å¾æˆ‘ä»¬è¦æƒ©ç½šï¼Œæˆ‘ä»¬å°†å¯¹æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼Œå¹¶ä¸”è®©ä»£ä»·å‡½æ•°æœ€ä¼˜åŒ–çš„è½¯ä»¶æ¥é€‰æ‹©è¿™äº›æƒ©ç½šçš„ç¨‹åº¦ã€‚è¿™æ ·çš„ç»“æœæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„èƒ½é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜çš„å‡è®¾ï¼š
$$
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]
$$
å…¶ä¸­ ğœ† åˆç§°ä¸º**æ­£åˆ™åŒ–å‚æ•°ï¼ˆRegularization Parameterï¼‰**ã€‚ æ ¹æ®æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä¸å¯¹ $ğœƒ_0â€‹$ è¿›è¡Œæƒ©ç½šã€‚

å¯¹ ğœ† çš„å–å€¼è¦åˆç†ï¼Œå¦‚æœ ğœ† è¿‡å¤§çš„è¯ï¼Œä¸ºäº†ä½¿å¾—ä»£ä»·å‡½æ•°å°½å¯èƒ½å°ï¼Œæ‰€æœ‰çš„ ğœƒ å€¼ï¼ˆä¸åŒ…æ‹¬ $ğœƒ_0$ï¼‰å°±éƒ½ä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°ï¼Œæˆ‘ä»¬åªèƒ½å¾—åˆ°ä¸€æ¡å¹³è¡Œäº ğ‘¥ è½´çš„ç›´çº¿ï¼Œè¿™æ ·ä¼šå¯¼è‡´**æ¬ æ‹Ÿåˆï¼ˆunderfitting)**ã€‚

â€‹                 

### 7.3. Regularized Linear Regression

å¯¹äºçº¿æ€§å›å½’çš„æ±‚è§£ï¼Œæˆ‘ä»¬ä¹‹å‰æ¨åˆ°äº†ä¸¤ç§å­¦ä¹ ç®—æ³•ï¼š

* æ¢¯åº¦ä¸‹é™
* æ­£è§„æ–¹ç¨‹

æ­£åˆ™åŒ–çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°ä¸ºï¼š
$$
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]
$$
å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»¤è¿™ä¸ªä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå…¶æ­£åˆ™åŒ–è¿­ä»£å¼å¦‚ä¸‹ï¼š
$$
\begin{align}
Repeat \{ \\
Î¸_0 &:= Î¸_0-\alpha\frac1m\sum^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})x^{(i)}_0 \\
Î¸_j &:= Î¸_j-\alpha \left[ \left( \frac1m\sum^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}Î¸_j \right] \ j\in\{1,2,\cdots,n\}\\
\}
\end{align}
$$
å°† $Î¸_0$ å•ç‹¬åˆ†å¼€çš„åŸå› ä¸Šé¢æåˆ°äº†ï¼Œå…¶å¯¹åº”çš„ç‰¹å¾é‡æ˜¯ 1ï¼Œçº¦å®šä¿—æˆä¸å¿…æ­£åˆ™åŒ–ã€‚ä¸Šå¼å¯åœ¨æ•´ç†ä¸ºï¼š
$$
Î¸_j := Î¸_j(1-\alpha\frac{\lambda}{m})-\alpha\frac1m\sum^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})x^{(i)}_j
$$
åœ¨è¿™ä¸ªå½¢å¼ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£å¾—åˆ°ï¼Œæ¯ä¸€æ¬¡è¿­ä»£éƒ½ä¼šå¤šä¹˜ä¸€ä¸ªå°äºä¸€çš„ç³»æ•° $1â€“Î±\fracÎ»m$ï¼Œè¿›è€Œç¼©å°ã€‚

â€‹              

å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ¥æ±‚è§£æ­£åˆ™åŒ–çº¿æ€§å›å½’æ¨¡å‹ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

![](https://photo.hushhw.cn/20190805165041.png)
$$
\theta = (X^TX + \lambda L)^{-1}X^Ty  \\
where \ L = \left[
\begin{matrix}
0 \qquad\qquad\qquad\qquad \\
\qquad 1  \qquad\qquad\qquad\\
\qquad\qquad1 \qquad\qquad \\
\qquad\qquad\qquad\ddots \qquad \\
\qquad\qquad\qquad\qquad 1
\end{matrix}
\right]
$$
 	å…¶ä¸­ L ä¸º(n+1)*(n+1)ã€‚

â€‹              

### 7.4. Regularized Logistic Regression

![](https://photo.hushhw.cn/20190811143857.png)

å¯¹äº logistic å›å½’æ¨¡å‹ï¼Œå¦‚æœä½ å¾—æ¨¡å‹ä¸­æœ‰å¾ˆå¤šçš„ç‰¹å¾ï¼Œæœ‰æ— å…³ç´§è¦çš„å¤šé¡¹å¼ï¼Œè¿™äº›å¤§é‡çš„ç‰¹å¾å°±ä¼šé€ æˆè¿‡æ‹Ÿåˆç°è±¡ã€‚

logistic å›å½’æ¨¡å‹æ­£åˆ™åŒ–åçš„ä»£ä»·å‡½æ•°ä¸ºï¼š
$$
J(\theta) = -\frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] + \frac{Î»}{2m}\sum^n_{j=1}\theta^2_j
$$
å¦‚æœä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æœ€å°åŒ–ä»£ä»·å‡½æ•°çš„è¯ï¼Œå…¶æ­£åˆ™åŒ–è¿­ä»£å¼ä¸ºï¼š
$$
\begin{align}
Repeat \{ \\
Î¸_0 &:= Î¸_0-\alpha\frac1m\sum^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})x^{(i)}_0 \\
Î¸_j &:= Î¸_j-\alpha \left[ \left( \frac1m\sum^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}Î¸_j \right] \ j\in\{1,2,\cdots,n\}\\
\}
\end{align}
$$
çœ‹ä¸Šå»å’Œçº¿æ€§å›å½’ä¸€æ ·ï¼Œä½†æ˜¯å› ä¸º $h_{\theta}(x) = g({\theta}^TX)$ï¼Œæ‰€ä»¥ä¸çº¿æ€§å›å½’ä¸åŒã€‚

â€‹                 

## ä¸“ä¸šåè¯æ•´ç†

* `overfitting`ï¼šè¿‡æ‹Ÿåˆã€`underfitting`ï¼šæ¬ æ‹Ÿåˆ

â€‹            

## å‚è€ƒ

> [é©¿èˆŸå°ç«™](https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-5/)
>
> [Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
>
> [æ–¯å¦ç¦å¤§å­¦ 2014 æœºå™¨å­¦ä¹ ](https://www.coursera.org/course/ml )

