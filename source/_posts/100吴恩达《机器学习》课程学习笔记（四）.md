---
title: 吴恩达《机器学习》笔记（四）——Logistic回归
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
abbrlink: f72c23ef
date: 2019-07-21 14:50:05
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
description: '本文内容介绍「logistic 回归」解决分类问题。'
---

> 本文内容介绍「logistic 回归」解决分类问题。



## Logistic Regression

### 6.1. Classification

与回归问题不同的是，分类问题中输出 y 不是连续的值，只能是 0 或者 1。并且 0 一般用来代表负向类（negative class），1 代表正向类（positive class），当然也可以任意指定。

如果我们要用线性回归算法来解决一个分类问题，对于分类， 𝑦 取值为 0 或者 1，那么假设函数的输出值可能远大于 1，或者远小于 0，即使所有训练样本的标签 𝑦 都等于 0 或 1。尽管我们知道标签应该取值 0 或者 1，但是如果算法得到的值远大于 1 或者远小于 0 的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做 **Logistic 回归**，这个算法的性质是：它的输出值永远在 0 到 1 之间。 

​        

### 6.2 Hypothesis Representation

在分类问题中，要用什么样的函数来表示我们的假设。希望我们的分类器的输出值在 0 和 1 之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在 0 和 1 之间，于是我们引入了一个新的模型——逻辑回归（Logistic Regression），该模型的输出变量范围式中在 0 和 1 之间。

logistic 回归模型的假设是：$h_{\theta}(x) = g(\theta^TX)$，其中 X 代表特征向量，g 代表 logistic 函数（logistic function）是一个常用的逻辑函数为 **sigmoid 函数（Sigmoid function）**，公式为：$g(z)= \frac1{1+e^{-z}} $，该函数图像如下图：

 ![](https://photo.hushhw.cn/20190721154842.png)

下面我们来理解一下 logistic 回归模型的假设：
$$
h_{\theta}(x) = \frac1{1+e^{-\theta^Tx}}
$$
其中，h 表示对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性（estimated probablity），即 $ℎ_𝜃(𝑥) = 𝑃(𝑦 = 1|𝑥;𝜃)​$。

例如，如果对于给定的 𝑥，通过已经确定的参数计算得出 $ℎ_𝜃(𝑥) = 0.7​$，则表示有 70% 的几率 𝑦 为正向类，相应地 𝑦 为负向类的几率为 1-0.7=0.3。 

​           

### 6.3. Decision Boundary

这一节讨论**决策边界（Decision Boundary）**问题，根据上面 sigmoid 函数的图像我们知道，当 $z\geq 0​$ 时 $g(z) \geq 0.5​$，反之小于 0.5，对应的 logistic 回归模型的假设得到：
$$
\theta^Tx \geq 0\ 时，预测\ y = 1\\
\theta^Tx < 0 \ 时，预测\ y=0
$$
假设有一个模型其假设函数为：$h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)​$，并且假设我们已经得到了参数 𝜃 的转置向量为 [-3 1 1]。则当 $-3 + x_1 + x_2 \geq 0​$，即 $x_1 + x_2 \geq 3​$ 时，模型将预测 y = 1。我们绘制出这条直线 $x_1 + x_2 = 3​$，这条线便是模型的分界线，即**决策边界**。

![](https://photo.hushhw.cn/20190721162039.png)

更进一步，若假设函数是非线性的，$h_θ=g(θ_0+θ_1x_1+θ_2x_2+θ_3x^2_1+θ_4x^2_2)​$，当我们通过算法得出 θ 后，画出图形如下，此时决策边界不一定是直线了：

![](https://photo.hushhw.cn/20190721162441.png)

​           

### 6.4. Cost Function

如何拟合 logistic 回归模型的参数 θ 呢？我们定义用来拟合参数的优化目标或者叫代价函数（cost function）。

在线性回归模型中，我们定义的代价函数是所有模型误差的平方和，即代价函数为：
$$
J(\theta) = \frac{1}{2m}\sum^m_{i=1}(h^{(i)}_{\theta}-y^{(i)})^2
$$
它的结构形式可以写成：
$$
J(\theta) = \frac{1}{m}\sum^m_{i=1}\frac12(h^{(i)}_{\theta}-y^{(i)})^2 \\
=\frac{1}{m}\sum^m_{i=1}Cost(h_{\theta}(x^{(i)}),y)
$$
理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于当我们将 $h_{\theta}(x) = \frac1{1+e^{-\theta^Tx}}$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个**非凸函数（non-convex）**。 

![](https://photo.hushhw.cn/20190731144737.png)

这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

这里 Andrew Ng 不加证明的重新给出了代价函数：
$$
Cost(h_{\theta}(x),y) = 
\begin{cases}
\begin{align}
-log(h_{\theta}(x)) \qquad if\quad y=1 \\
-log(1-h_{\theta}(x)) \qquad if\quad y=0
\end{align}
\end{cases}
$$
$h_{\theta}(x)​$ 与代价函数之间的关系如下图：

![](https://photo.hushhw.cn/20190731150435.png)

![](https://photo.hushhw.cn/20190731150528.png)

这样构建的代价函数的特点是：当实际的 y = 1 时，$h_{\theta}(x)$ 为 1 时误差为 0，当 $h_{\theta}(x)$ 不为 1 时误差随着趋于 0 而变大；当实际的 y = 0 时，$h_{\theta}(x)$ 为 0 时误差为 0，当 $h_{\theta}(x)​$ 不为 0 时误差随着趋于 1 而变大。

由于 y 只能取 0 或 1，可以把这个式子整合成：
$$
Cost(h_{\theta}(x), y) = -ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))
$$
即 logistic 回归的代价函数为：
$$
\begin{align}
J(\theta) &= \frac{1}{m}\sum^{m}_{i=1}Cost(h_{\theta}(x^{(i)}),y^{(i)}) \\
&= -\frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right]
\end{align}
$$

​            

### 6.5. Gradient Descent

在得到这样一个代价函数后，我们便可以用**梯度下降算法**来求得使代价函数最小的参数了，即最小化 J(𝜃)。

梯度下降算法前面我们已经用过，用循环运算表达的形式如下：
$$
\theta_j:=\theta_j-\alpha\frac{∂}{∂\theta_j}J(\theta)
$$
用上面的式子来不断更新所有的 𝜃 值，如果将 J(θ) 的偏导数带入后，我们得到：
$$
\theta_j:=\theta_j-\alpha\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j
$$
现在，如果你把这个更新规则和我们之前用在线性回归上的进行比较的话，你会惊讶地发现，这个式子正是我们用来做线性回归梯度下降的。 但是线性回归和 logistic 回归并不一样，因为两者的假设函数 $h_{\theta}(x)$ 不一样。

------

**代价函数偏导数的过程**

首先计算 sigmoid 函数的导数：
$$
\begin{split}
σ(x)^{'} &=(\frac{1}{1+e^{-x}})^{'} 
=\frac{e^{-x}}{(1+e^{-x})^2}=(\frac{1}{1+e^{-x}})(\frac{e^{-x}}{1+e^{-x}})=σ(x)(\frac{1+e^{-x}-1}{1+e^{-x}}) \\
&= σ(x)(1-σ(x))
\end{split}
$$
再来对代价函数求偏导：
$$
\begin{align}
\frac{∂}{∂θ_j}J(θ) 
&= \frac{∂}{∂θ_j}\frac{-1}{m}\sum^{m}_{i=1}\left[y^{(i)}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}\frac{∂}{∂θ_j}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})\frac{∂}{∂θ_j}log(1-h_{\theta}(x^{(i)}))\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}\frac{\frac{∂}{∂θ_j}h_{\theta}(x^{(i)})}{h_{\theta}(x^{(i)})} + (1-y^{(i)})\frac{\frac{∂}{∂θ_j}(1-h_{\theta}(x^{(i)}))}{1-h_{\theta}(x^{(i)})}\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}\frac{\frac{∂}{∂θ_j}σ({\theta}^Tx^{(i)})}{σ({\theta}^Tx^{(i)})} + (1-y^{(i)})\frac{\frac{∂}{∂θ_j}(1-σ({\theta}^Tx^{(i)}))}{1-σ({\theta}^Tx^{(i)})}\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}\frac{σ(1-σ)\frac{∂}{∂θ_j}({\theta}^Tx^{(i)})}{σ} + (1-y^{(i)})\frac{σ(1-σ)\frac{∂}{∂θ_j}(1-{\theta}^Tx^{(i)})}{1-σ}\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}(1-σ)x^{(i)}_j - (1-y^{(i)})σx^{(i)}_j\right] \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}(1-h_{\theta}(x^{(i)})) - (1-y^{(i)})h_{\theta}(x^{(i)})\right]x^{(i)}_j \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)}-y^{(i)}h_{\theta}(x^{(i)}) - h_{\theta}(x^{(i)})+y^{(i)}h_{\theta}(x^{(i)})\right]x^{(i)}_j \\
&= -\frac1m\sum^{m}_{i=1}\left[y^{(i)} - h_{\theta}(x^{(i)})\right]x^{(i)}_j \\
&= \frac1m\sum^{m}_{i=1}\left[h_{\theta}(x^{(i)})-y^{(i)}\right]x^{(i)}_j \\
\end{align}
$$
​             

### 6.6. Advanced Optimization

上面我们讨论了用梯度下降的方法最小化逻辑回归中代价函数 𝐽(𝜃)。这一节介绍一些一些高级优化算法和一些高级的优化概念，利用这些方法我们就能够使通过梯度下降进行 logistic 回归的速度大大提高，而这也将使算法更加适合解决大型的机器学习问题。

 如果我们能用这些方法来计算代价函数 𝐽(𝜃) 和偏导数项的话，那么这些算法就是为我们优化代价函数的不同方法，**共轭梯度法**、**BFGS（变尺度法）**和 **L-BFGS（限制变尺度法）**就是其中一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃) 以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。

他们相对比较复杂，优势主要有两点

* 不需要选择学习率α，算法会自适应
* 经常比梯度下降算法快

​           

### 6.7. Multi-class classification: One-vs-all

前面我们讨论的分类问题都是**二元分类（Binary classification）**，这一节介绍使用 logistic 回归来解决**多类别分类（Multi-class classfication）**问题，具体来说讨论「一对多」的分类问题。

![](https://photo.hushhw.cn/20190801162235.png)

解决多类别分类问题的思路是将其分成多个二元分类问题。

![](https://photo.hushhw.cn/20190801162837.png)

​                               

## 专业名词整理

* `dependent variable`：因变量
* `convex function`：凸函数、`non-convex function`：非凸函数
* `maximum likelihood estimation`：极大似然估计
* `binary classification`：二元分类、`multi-class classfication`：多类别分类

​             

## 参考

> [驿舟小站](https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-3/)
>
> [Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
>
> [斯坦福大学 2014 机器学习](https://www.coursera.org/course/ml )