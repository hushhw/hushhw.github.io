---
title: 吴恩达《机器学习》笔记（七）——神经网络的学习
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
abbrlink: 18a3eba2
date: 2019-08-13 17:14:21
---



## Neural Networks： Learning

### 9.1. Cost Function

首先引入一些标记：

* $L$：神经网络的总层数
* $s_l$：第 L 层的单元数量（不含偏置单元）
* $K$：输出单元的数量
* $h_{\Theta}(x)_k$：假设函数中的第 k 个输出

下图中分别为单类和多类分类的表示形式：

![](https://photo.hushhw.cn/20190905174159.png)

在 logistic 回归问题中我们的代价函数是：
$$
J(\theta) = -\frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] + \frac{λ}{2m}\sum^n_{j=1}\theta^2_j
$$

而神经网络中的代价函数是 logistic 回归代价函数的推广，在 logistic 回归中只有一个输出变量且只有一个因变量 y，而在神经网络中却有很多输出变量，其代价函数要复杂一些，如下：
$$
\begin{align}
J(\theta) &= -\frac{1}{m}\left[\sum^{m}_{i=1}\sum^k_{k=1}y_k^{(i)}log\left( h_{\Theta}(x^{(i)})\right)_k + (1-y_k^{(i)})log\left(1-h_{\Theta}(x^{(i)})\right)_k\right] \\
&+ \frac{λ}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta^{(l)}_{j,i})^2
\end{align}
$$


### 9.2. Backpropagation Algorithm

待补充。。。