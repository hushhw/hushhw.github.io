---
title: å´æ©è¾¾ã€Šæœºå™¨å­¦ä¹ ã€‹ç¬”è®°ï¼ˆä¸‰ï¼‰â€”â€”å¤šå˜é‡çº¿æ€§å›å½’
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
tags:
  - Machine Learning
categories:
  - ç¬”è®°æ•´ç†
  - æœºå™¨å­¦ä¹ 
abbrlink: 92253a1a
date: 2019-07-16 16:49:45
description: 'æœ¬æ–‡å†…å®¹ä»‹ç»ã€Œå¤šå˜é‡çº¿æ€§å›å½’ã€é—®é¢˜ã€‚é¦–å…ˆå¼•å…¥ä¸€ä¸ªå¤šç»´ç‰¹å¾æ¨¡å‹ï¼Œåˆ©ç”¨ã€Œå¤šå˜é‡æ¢¯åº¦ä¸‹é™ã€æ¥è¿­ä»£æ±‚å‡ºä»£ä»·å‡½æ•°æœ€å°å€¼ã€‚ä¹‹åä»‹ç»äº†ã€Œå¤šé¡¹å¼å›å½’ã€é—®é¢˜ï¼Œä»¥åŠã€Œæ­£è§„æ–¹ç¨‹ã€ï¼Œå¹¶æŠŠæ­£è§„æ–¹ç¨‹å’Œæ¢¯åº¦ä¸‹é™é—®é¢˜åšäº†æ¯”è¾ƒã€‚'
---

> æœ¬æ–‡å†…å®¹ä»‹ç»ã€Œå¤šå˜é‡çº¿æ€§å›å½’ã€é—®é¢˜ã€‚
>
> é¦–å…ˆå¼•å…¥ä¸€ä¸ªå¤šç»´ç‰¹å¾æ¨¡å‹ï¼Œåˆ©ç”¨ã€Œå¤šå˜é‡æ¢¯åº¦ä¸‹é™ã€æ¥è¿­ä»£æ±‚å‡ºä»£ä»·å‡½æ•°æœ€å°å€¼ã€‚
>
> ä¹‹åä»‹ç»äº†ã€Œå¤šé¡¹å¼å›å½’ã€é—®é¢˜ï¼Œä»¥åŠã€Œæ­£è§„æ–¹ç¨‹ã€ï¼Œå¹¶æŠŠæ­£è§„æ–¹ç¨‹å’Œæ¢¯åº¦ä¸‹é™é—®é¢˜åšäº†æ¯”è¾ƒã€‚



## Linear Algebra Review

è¿™éƒ¨åˆ†çº¿æ€§ä»£æ•°çš„åªæ˜¯éå¸¸åŸºç¡€ï¼Œä»…ä»…æ•´ç†äº†ä¸€äº›ä¸“ä¸šåè¯çš„è‹±æ–‡å»ç†Ÿæ‚‰å­¦ä¹ ã€‚

â€‹        

## Linear Regression with Multiple Variables

### 4.1. Multiple Features

å‰é¢æˆ‘ä»¬è®¨è®ºäº†å•å˜é‡ç‰¹å¾ï¼ˆsingle variableï¼‰çš„å›å½’æ¨¡å‹ï¼ˆå½±å“æˆ¿å±‹ä»·æ ¼çš„å› ç´ ä»…æœ‰é¢ç§¯å¤§å°ï¼‰ï¼Œç°åœ¨æˆ‘ä»¬å¯¹æˆ¿ä»·æ¨¡å‹å¢åŠ æ›´å¤šçš„ç‰¹å¾ $(x_1,x_2,...,x_n)$ï¼Œæ„æˆä¸€ä¸ªå¤šå˜é‡çš„æ¨¡å‹ã€‚

ä¸‹é¢è¿™ä¸ªä¾‹å­ä¸ºå››ä¸ªç‰¹å¾çš„æ¨¡å‹ï¼Œå…¶ä¸­ä¸€äº›å‚æ•°çš„è¯´æ˜å¦‚å›¾ï¼š

![](https://photo.hushhw.cn/20190716173040.png)

æ”¯æŒå¤šå˜é‡çš„å‡è®¾ â„ è¡¨ç¤ºä¸ºï¼š$â„_{\theta}(ğ‘¥) = \theta_0 + \theta_1x_1 + \theta_2x_2+...+\theta_ğ‘›x_ğ‘›â€‹$ ï¼Œè¿™ä¸ªå…¬å¼ä¸­æœ‰ ğ‘› + 1 ä¸ªå‚æ•°å’Œ ğ‘› ä¸ªå˜é‡ï¼Œä¸ºäº†ä½¿å¾—å…¬å¼èƒ½å¤Ÿç®€åŒ–ä¸€äº›ï¼Œå¼•å…¥ $x_0 = 1â€‹$ï¼Œåˆ™å…¬å¼è½¬åŒ–ä¸ºï¼š$â„_{\theta}(ğ‘¥) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2+...+\theta_ğ‘›x_ğ‘›â€‹$ ã€‚

æ”¹å†™æˆçŸ©é˜µçš„å½¢å¼ï¼š
$$
h_{\theta}(x) = \left[
\begin{matrix}
\theta_0 & \theta_1 & \theta_2 \cdots & \theta_n 
\end{matrix}
\right]
\left[
\begin{matrix}
x_0 \\ x_1 \\x_2 \\ \vdots\\ x_n
\end{matrix}
\right]
 = \theta^TX
$$
æ­¤æ—¶æ¨¡å‹ä¸­çš„å‚æ•°æ˜¯ä¸€ä¸ª ğ‘› + 1 ç»´çš„å‘é‡ï¼Œä»»ä½•ä¸€ä¸ªè®­ç»ƒå®ä¾‹ä¹Ÿéƒ½æ˜¯ ğ‘› + 1 ç»´çš„å‘é‡ï¼Œç›¸ä¹˜å°±ç›¸å½“äºå‚æ•°å‘é‡çš„è½¬ç½®ä¹˜ä»¥å®ä¾‹å‘é‡ï¼Œå› æ­¤å…¬å¼å¯ä»¥ç®€åŒ–ä¸ºï¼š
$$
h_{\theta}(x) = \theta^TX
$$
â€‹        

### 4.2. Gradient Descent for Multiple Variables

ä¸å•å˜é‡çº¿æ€§å›å½’ç±»ä¼¼ï¼Œåœ¨å¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼ˆcost functionï¼‰ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œï¼Œå³ï¼š
$$
J(\theta_0,\theta_1,\cdots,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
æˆ‘ä»¬å¯ä»¥ç®€å†™æˆ $J(\theta)$ ï¼Œå…¶ä¸­ï¼š$h_{\theta}(x) = \theta^TX = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n$

æˆ‘ä»¬çš„ç›®æ ‡å’Œå•å˜é‡çº¿æ€§å›å½’é—®é¢˜ä¸€æ ·ï¼Œè¦æ‰¾åˆ°ä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„ç³»åˆ—å‚æ•°ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ¥æ±‚ï¼Œå°±æ˜¯è¦ä¸æ–­çš„æ›´æ–°æ¯ä¸€ä¸ª $\theta_j$ å‚æ•°ï¼Œé€šè¿‡ $\theta_j$ å‡å» $\alpha$ ä¹˜ä»¥å¯¼æ•°é¡¹ï¼Œå…·ä½“è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼Œåˆ†åˆ«æ˜¯å•ç‰¹å¾å’Œæœ‰å¤šä¸ªç‰¹å¾çš„æ¨¡å‹çš„æƒ…å†µã€‚

![](https://photo.hushhw.cn/20190716183159.png)

â€‹          

### 4.3. Feature Scaling

æœºå™¨å­¦ä¹ ä¸­æˆ‘ä»¬ä½¿ç”¨å‚æ•°çš„ç›®æ ‡æ˜¯ä¸ºäº†é€šè¿‡æ•°æ®é›†æ”¹å–„æ¯ä¸€ä¸ªç‰¹å¾é‡ x å¯¹åº”çš„æƒé‡ Î¸ï¼Œæœ€ç»ˆå¾—åˆ°æ‹Ÿåˆåº¦é«˜çš„å‡è®¾æ–¹ç¨‹ã€‚è¿™æ—¶å°±å‡ºç°äº†ä¸€ç§æƒ…å†µï¼Œå¦‚æœæŸä¸ªç‰¹å¾é‡çš„æ•°é‡çº§è¿œå¤§äºå…¶å®ƒç‰¹å¾é‡ï¼Œå°±ä¼šå¯¼è‡´è¯¥ç‰¹å¾åœ¨å­¦ä¹ ç®—æ³•ä¸­å ä¸»å¯¼ä½ç½®ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡é¢„å¤„ç†çš„æ–¹å¼è®©åˆå§‹çš„ç‰¹å¾é‡å…·æœ‰åŒç­‰çš„åœ°ä½ï¼Œæ‰èƒ½è®©æœºå™¨å­¦ä¹ ç®—æ³•æ›´å¿«åœ°å­¦ä¹ å¾—åˆ°ä»–ä»¬çš„æƒé‡ã€‚

ä½¿ç”¨è¿™é‡Œã€Œæˆ¿ä»·é¢„ä¼°ã€çš„ä¾‹å­ï¼Œå‡è®¾æœ‰ä¸¤ä¸ªç‰¹å¾å½±å“æˆ¿ä»·ï¼Œåˆ†åˆ«æ˜¯ã€Œé¢ç§¯ã€å’Œã€Œæˆ¿é—´æ•°ã€ï¼Œå¾ˆæ˜¾ç„¶è¿™ä¸¤ä¸ªæ•°æ®å·®åˆ«æ˜¯éå¸¸å¤§çš„ï¼Œåœ¨æ²¡æœ‰è¿›è¡Œå•ä½ç»Ÿä¸€ä¹‹å‰ï¼Œç”±äºå˜é‡çš„å•ä½ç›¸å·®å¾ˆå¤§ï¼Œå¯¼è‡´äº†æ¤­åœ†å‹çš„æ¢¯åº¦è½®å»“ï¼›ç¼©æ”¾ä¹‹åå°†å˜é‡å˜æˆåŒä¸€å•ä½ï¼Œäº§ç”Ÿäº†åœ†å½¢è½®å»“ã€‚

![](https://photo.hushhw.cn/20190721103703.png)

![å›¾ç‰‡æ¥æºï¼šTingMind.cn](https://photo.hushhw.cn/20190721103741.png)

ç”±äºæ¢¯åº¦ä¸‹é™æ˜¯æŒ‰åˆ‡çº¿æ–¹å‘ä¸‹é™ï¼Œæ‰€ä»¥å¯¼è‡´äº†ç³»ç»Ÿåœ¨æ¤­åœ†è½®å»“ä¸åœè¿‚å›åœ°å¯»æ‰¾æœ€ä¼˜è§£ï¼Œè€Œåœ†å½¢è½®å»“å°±èƒ½è½»æ¾æ‰¾åˆ°ã€‚

ç‰¹å¾ç¼©æ”¾çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ä½¿ç”¨ $x_n = \frac{xn-\mu}{max-min}$ çš„æ–¹å¼ï¼Œå°†æ•°æ®ç¼©æ”¾ä¸º [-1, 1] çš„èŒƒå›´ï¼Œä¾‹å¦‚æˆ¿å±‹é¢ç§¯èŒƒå›´åœ¨ [100, 2000]ï¼Œå¹³å‡å€¼ä¸º 1000ï¼Œå°±å¯ä»¥è®¡ç®— $x_{new}= \frac{x-1000}{1900}$ã€‚

æ›´ä¸€èˆ¬çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨**å‡å€¼å½’ä¸€åŒ–ï¼ˆmean normalizationï¼‰**ä½¿å¾—æ•°æ®é›†çš„å¹³å‡å€¼ä¸ºé›¶ï¼š$x_n = \frac{x_n-\mu_n}{s_n}$ï¼Œå…¶ä¸­ $ğœ‡_ğ‘›$ æ˜¯å¹³å‡å€¼ï¼Œ$ğ‘ _ğ‘›$ æ˜¯æ ‡å‡†å·®ã€‚ 

> åŸ‹å‘ï¼šè¿™é‡Œéœ€è¦å»ç†è§£ç‰¹å¾æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–çš„åŒºåˆ«ï¼Œæœ‰å¾…æ·±å…¥å­¦ä¹ ã€‚

â€‹        

### 4.4. Learning Rate

æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚ ä¹Ÿæœ‰ä¸€äº›è‡ªåŠ¨æµ‹è¯•æ˜¯å¦æ”¶æ•›çš„æ–¹æ³•ï¼Œä¾‹å¦‚å°†ä»£ä»·å‡½æ•°çš„å˜åŒ–å€¼ä¸æŸä¸ªé˜€å€¼ï¼ˆä¾‹å¦‚ 0.001ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚

æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ¯æ¬¡è¿­ä»£å—åˆ°å­¦ä¹ ç‡çš„å½±å“ï¼Œå¦‚æœå­¦ä¹ ç‡ ğ‘ è¿‡å°ï¼Œåˆ™è¾¾åˆ°æ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ä¼šéå¸¸é«˜ï¼›å¦‚æœå­¦ä¹ ç‡ ğ‘ è¿‡å¤§ï¼Œæ¯æ¬¡è¿­ä»£å¯èƒ½ä¸ä¼šå‡å°ä»£ä»·å‡½æ•°ï¼Œå¯èƒ½ä¼šè¶Šè¿‡å±€éƒ¨æœ€
å°å€¼å¯¼è‡´æ— æ³•æ”¶æ•›ã€‚

â€‹         

### 4.5. Polynomial Regression

å‰é¢å­¦ä¹ äº†çº¿æ€§å›å½’ï¼Œä½†æ˜¯çº¿æ€§å›å½’å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰æ•°æ®ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦æ›²çº¿æ¥é€‚åº”æˆ‘ä»¬çš„æ•°æ®ï¼Œè¿™ä¸€èŠ‚ç»§ç»­å­¦ä¹ **å¤šé¡¹å¼å›å½’ï¼ˆPolynomial Regressionï¼‰**ï¼Œæ¯”å¦‚ä¸€ä¸ªä¸‰æ¬¡æ–¹æ¨¡å‹ï¼š$h_{\theta}(x) = \theta_0+\theta_1x_1+\theta_2x^2_2+\theta_3x_3^3$ï¼Œä¸‹å›¾æ˜¯ä¸€ä¸ªä¸€å…ƒå¤šé¡¹å¼å›å½’çš„ä¾‹å­ã€‚

![](https://photo.hushhw.cn/20190721123805.png)

å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œåœ¨è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•å‰ï¼Œç‰¹å¾ç¼©æ”¾éå¸¸æœ‰å¿…è¦ã€‚ 

â€‹            

### 4.6. Normal Equation

åœ¨å‰é¢æˆ‘ä»¬å­¦ä¹ äº†ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥è®¡ç®—å‚æ•°æœ€ä¼˜è§£ï¼Œå…¶è¿‡ç¨‹æ˜¯å¯¹ä»£ä»·å‡½æ•°ç›¸å¯¹äºæ¯ä¸ªå‚æ•°æ±‚åå¯¼æ•°ï¼Œé€šè¿‡è¿­ä»£ç®—æ³•ä¸€æ­¥ä¸€æ­¥è¿›è¡ŒåŒæ­¥æ›´æ–°ï¼Œç›´åˆ°æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ï¼Œä»è€Œå¾—åˆ°æœ€ä¼˜å‚æ•°å€¼ã€‚ä½†æ˜¯å¯¹äºæŸäº›çº¿æ€§å›å½’é—®é¢˜ï¼Œ**æ­£è§„æ–¹ç¨‹ï¼ˆNormal Equationï¼‰**æ–¹æ³•æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œæ­£è§„æ–¹ç¨‹åˆ™æ˜¯é€šè¿‡æ•°å­¦æ–¹æ³•ä¸€æ¬¡æ€§æ±‚å¾—æœ€ä¼˜è§£ã€‚

ä¸‹é¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ 4 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªçŸ©é˜µ X åŒ…å«è®­ç»ƒæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾å˜é‡ï¼Œæ„æˆä¸€ä¸ª m*(n+1) ç»´çŸ©é˜µï¼Œæ‰€æœ‰çš„é¢„æµ‹å€¼æ”¾å…¥å‘é‡ yï¼Œæ„æˆä¸€ä¸ª m ç»´å‘é‡ã€‚ä»£ä»·å‡½æ•°æœ€å°åŒ–çš„ $\theta = (X^TX)^{-1}X^Tyâ€‹$ã€‚

![](https://photo.hushhw.cn/20190721130118.png)

å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œå‡è®¾æœ‰ m ç»„è®­ç»ƒæ ·æœ¬åˆ†åˆ«ä¸º $(x^{(1)},y^{(1)},\cdots,x^{(m)},y^{(m)})$ï¼Œæ¯ç»„è®­ç»ƒæ ·æœ¬æœ‰ n ä¸ªç‰¹å¾å€¼ã€‚å…¶ä¸­ï¼Œ
$$
x^{(i)} = 
\left[
	\begin{matrix}
	x_0^{(i)}\\x_1^{(i)}\\x_2^{(i)}\\\vdots\\x_n^{(i)}
	\end{matrix}
\right]
\in\R^{n+1}
,å¯¹åº”çš„ X = \left[
	\begin{matrix}
	\cdots {x^{(1)}}^T \cdots \\
	\cdots {x^{(2)}}^T \cdots \\
	\vdots  \\
	\cdots {x^{(m)}}^T \cdots \\
	\end{matrix}
\right]
$$
å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µï¼ˆé€šå¸¸æ˜¯å› ä¸ºç‰¹å¾ä¹‹é—´ä¸ç‹¬ç«‹ï¼Œå¦‚åŒæ—¶åŒ…å«è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸å’Œç±³ä¸ºå•ä½çš„å°ºå¯¸ä¸¤ä¸ªç‰¹å¾ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ç‰¹å¾æ•°é‡å¤§äºè®­ç»ƒé›†çš„æ•°é‡ï¼‰ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚ 

æ¢¯åº¦ä¸‹é™ä¸æ­£è§„æ–¹ç¨‹æ¯”è¾ƒï¼š

|           æ¢¯åº¦ä¸‹é™            |                           æ­£è§„æ–¹ç¨‹                           |
| :---------------------------: | :----------------------------------------------------------: |
|        éœ€è¦é€‰æ‹©å­¦ä¹ ç‡ğ›¼        |                            ä¸éœ€è¦                            |
|         éœ€è¦å¤šæ¬¡è¿­ä»£          |                         ä¸€æ¬¡è¿ç®—å¾—å‡º                         |
| å½“ç‰¹å¾æ•°é‡ ğ‘› å¤§æ—¶ä¹Ÿèƒ½è¾ƒå¥½é€‚ç”¨ | éœ€è¦è®¡ç®— ${(X^TX)}^{-1}$ å¦‚æœç‰¹å¾æ•°é‡ ğ‘› è¾ƒå¤§åˆ™è¿ç®—ä»£ä»·å¤§ï¼Œå› ä¸ºçŸ©é˜µé€†çš„è®¡ç®—æ—¶é—´å¤æ‚åº¦ ä¸º $ğ‘‚(ğ‘›^3)$ï¼Œé€šå¸¸æ¥è¯´å½“ ğ‘› å°äº 10000 æ—¶è¿˜æ˜¯å¯ä»¥æ¥å—çš„ |
|     é€‚ç”¨äºå„ç§ç±»å‹çš„æ¨¡å‹      |        åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹        |

åªè¦ç‰¹å¾å˜é‡çš„æ•°ç›®å¹¶ä¸å¤§ï¼Œæ ‡å‡†æ–¹ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è®¡ç®—å‚æ•°ğœƒçš„æ›¿ä»£æ–¹æ³•ã€‚å…·ä½“åœ°è¯´ï¼Œåªè¦ç‰¹å¾å˜é‡æ•°é‡å°äºä¸€ä¸‡ï¼Œæˆ‘é€šå¸¸ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ï¼Œè€Œä¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚ 
éšç€æˆ‘ä»¬è¦è®²çš„å­¦ä¹ ç®—æ³•è¶Šæ¥è¶Šå¤æ‚ï¼Œä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬è®²åˆ°åˆ†ç±»ç®—æ³•ï¼Œåƒé€»è¾‘å›å½’ç®—æ³•ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œå®é™…ä¸Šå¯¹äºé‚£äº›ç®—æ³•ï¼Œå¹¶ä¸èƒ½ä½¿ç”¨æ ‡å‡†æ–¹ç¨‹æ³•ã€‚å¯¹äºé‚£äº›æ›´å¤æ‚çš„å­¦ä¹ ç®—æ³•ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»ç„¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ã€‚å› æ­¤ï¼Œæ¢¯åº¦ä¸‹é™æ³•æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„ç®—æ³•ï¼Œå¯ä»¥ç”¨åœ¨æœ‰å¤§é‡ç‰¹å¾å˜é‡çš„çº¿æ€§å›å½’é—®é¢˜ã€‚

â€‹         

## ä¸“ä¸šåè¯æ•´ç†

* `linear algebra`ï¼šçº¿æ€§ä»£æ•°
* `matrix(matrices)`ï¼šçŸ©é˜µ
* `vector`ï¼šå‘é‡
* `two dimensional array`ï¼šäºŒç»´æ•°ç»„
* `multiplication`ï¼šä¹˜æ³•
* `inverse`ï¼šé€†ã€`transpose`ï¼šè½¬ç½®
* `superscript`ï¼šä¸Šæ ‡ã€`subscript`ï¼šä¸‹æ ‡
* `feature scaling`ï¼šç‰¹å¾æ”¾ç¼©
* `standard deviation`ï¼šæ ‡å‡†å·®
* `average value`ï¼šå¹³å‡å€¼
* `polynomial`ï¼šå¤šé¡¹å¼
* `normal equation`ï¼šæ­£è§„æ–¹ç¨‹



## å‚è€ƒ

> [æ•°æ®ç‰¹å¾ æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–ä½ äº†è§£å¤šå°‘ï¼Ÿ](https://www.tinymind.cn/articles/1217)
>
> [é©¿èˆŸå°ç«™](https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-3/)
>
> [Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
>
> [æ–¯å¦ç¦å¤§å­¦ 2014 æœºå™¨å­¦ä¹ ](https://www.coursera.org/course/ml )