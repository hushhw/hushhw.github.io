---
title: 吴恩达《机器学习》课程学习笔记（二）
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
abbrlink: df3b9948
date: 2019-07-16 11:49:49
---



## Linear Regression with One Variable

### 2.1 Univariate linear regression

* $𝑚$ 代表训练集中实例的数量 
* $𝑥$ 代表特征/输入变量 
* $𝑦​$ 代表目标变量/输出变量 
* $(𝑥,𝑦)​$ 代表训练集中的实例 
* $(𝑥^{(𝑖)},𝑦^{(𝑖)})$ 代表第𝑖 个观察实例 
* $ℎ$ 代表学习算法的解决方案或函数也称为假设（**hypothesis**） 

这里以房屋价格预测为例，我们的学习算法中，我么需要通过学习得到一个假设 h，h 表示一个函数，输入房屋尺寸大小 x，从而得到输出房屋的价格 y，因此 h 是一个从 x 到 y 的函数映射。
![](https://photo.hushhw.cn/20190716121509.png)

这里我们采用 $h_{\theta}(x) = \theta_0 + \theta_1x$ 来表达 h，因为只含有一个特征/输入变量，因此被称为**单变量线性回归问题（Univariate linear regression）**。

​           

### 2.2 Cost function

我们前面得到了预测的函数是一个线性函数：$h_{\theta}(x) = \theta_0 + \theta_1x$，下面我们需要选择合适的参数（parameters）$\theta_0$ 和 $\theta_1$ 来使得我们的预测更加准确，模型所预测的值和训练集中实际值之间的差距就是**建模误差（modeling error）**。

一般而言，我们通过调整 θ，使得所有训练集数据与其拟合数据的差的平方和更小，即认为得到了拟合度更好的函数。从而我们得到**代价函数（cost function）**：
$$
J（\theta_0,\theta_1) = \frac{1}{2m}\sum^m_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
这里的$\frac{1}{2}$ 是为了方便后面的运算而加上的。

代价函数又被称为**平方误差函数**，有时也被称为**平方误差代价函数**。求误差的平方在大多数问题，特别是回归问题都是一个合理的选择。

回顾一下目前为止我们的思路：

![](https://photo.hushhw.cn/20190716133921.png)

​          

### 2.3 Gradient Descent

本节来解决如何求 $minimize J(\theta_0, \theta_1)$ 的问题。

**梯度下降（Gradient Descent)** 是一个用来求函数最小值的算法，我们将使用梯度下降算法求出代价函数最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合 $(𝜃_0,𝜃_1,...,𝜃_𝑛)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 

![](https://photo.hushhw.cn/20190716135043.png)

![](https://photo.hushhw.cn/20190716135116.png)

上面是一个非线性函数的代价函数，我们可以看到，选取不同的初始值 θ，可能会使得迭代的代价函数最后进入不同的极小值点。

梯度下降算法公式：

![](https://photo.hushhw.cn/20190716135452.png)

其中，

* $\alpha$ 是**学习率（learning rate）**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。$\alpha​$ 不能太大也不能太小，太小了会使得移动的速率很慢，需要很多步才能到达全局最低点。太大了可能会直接越过了最低点，甚至可能无法收敛，甚至发散。
* := 表示赋值（assignment）

对于单变量线性回归的梯度函数而言，其代价函数 J 关于参数 θ的图像如下，只有一个极值以及最值：

![](https://photo.hushhw.cn/20190716141637.png)

我们让模型再简化一下，取J(θ)=θx,其代价函数关于θ的图像如下。我们通过观察他的迭代过程，有助于理解梯度下降算法：

![](https://photo.hushhw.cn/20190716141721.png)

可以看到：

* 当 θ 大于最小值时，导数为正，那么迭代公式 $\theta := \theta - \alpha\frac{∂}{∂{\theta}}J(\theta)​$ 里，θ 减去一个正数，向左往最小值逼近；
* 当θ小于最小值时，导数为负，那么迭代公式 $\theta := \theta - \alpha\frac{∂}{∂{\theta}}J(\theta)$ 里，θ 减去一个负数，向右往最小值逼近。

​          

## 专业名词整理

* `hypothesis`：假设
* `parameters`：参数
* `Gradient Descent`：梯度下降
* `Univariate linear regression`：单变量线性回归
* `cost function`：代价函数
* `learning rate`：学习率

* `derivative term`：导数项、`partial derivative`：偏导数
* `equation`：方程式
* `positive slope`：正斜率、`negative slope`：负斜率





## 参考

> [驿舟小站](https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-2/)
>
> [Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
>
> [斯坦福大学 2014 机器学习](