---
title: 吴恩达《机器学习》笔记（九）——支持向量机SVM
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
date: 2019-08-23 12:32:50
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
---



## Support Vector Machines

### 12.1. Optimization Objective

**支持向量机（Support Vector Machines，SVM）**在学习复杂的非线性方程时提供了一种更为清晰更为强大的方式，属于监督学习算法。

为了描述 SVM，我们从 logistic 回归开始，下图是回顾 logistic 回归的假设函数的一些特性：

![](https://photo.hushhw.cn/20190906130133.png)

再看看 logistic 回归的代价函数：
$$
\begin{align}
J(\theta) &= \frac{1}{m}\sum^{m}_{i=1}\left[-y^{(i)}log\ h_{\theta}(x^{(i)}) - (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] \\
&= \frac{1}{m}\sum^{m}_{i=1}\left[-y^{(i)}log\left( \frac1{1+e^{-\theta^Tx^{(i)}}}\right) - (1-y^{(i)})log\left(1-\frac1{1+e^{-\theta^Tx^{(i)}}}\right)\right]
\end{align}
$$
令 $z=\theta^Tx​$ 后画出两条光滑的 sigmoid 函数曲线如下图：

![](https://photo.hushhw.cn/20190906132048.png)

用两条粉色线段代替曲线，这被称为 hinge loss 函数。

* 当 y = 1 时，命名为 $cost_1(z)$ ，当 z 大于 1 时为零，小于 1 时斜率可以不用考虑。

* 当 y = 0 时，命名为 $cost_0(z)$ ，当 z 小于 -1 时为零，大于 -1 时斜率可以不用考虑。

logistic 回归正则化后的待见函数为：
$$
J(\theta) = \frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}\left(-log\ h_{\theta}(x^{(i)})\right) + (1-y^{(i)})\left(-log(1-h_{\theta}(x^{(i)}))\right)\right] + \frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j 
$$
用命名后的线段代替 logistic 回归代价函数的第一项和第二项得到：
$$
J(\theta) = \frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j 
$$
现在我们考虑去掉 1/m 这一项，去掉后依然可以得到同样的 𝜃 最优值，因为 1/m 仅是一个常量。我们做一下变形，乘以一个 m/λ：
$$
J(\theta) = \frac{1}{\lambda}\sum^{m}_{i=1}\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac12\sum^n_{j=1}\theta^2_j
$$
最后我们记 C=1/λ，得到：
$$
J(\theta) = C\sum^{m}_{i=1}\left[y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost_0(\theta^Tx^{(i)})\right] + \frac12\sum^n_{j=1}\theta^2_j
$$
这就是通常使用的 SVM 代价函数，这个系数 C 本质上和 λ 一样，都是改变普通代价函数项和正则项的权重关系。也就是说，如果我们想要加强正则化强度来处理过拟合，那么减小 C；如果想要减少正则化强度来处理欠拟合，那么增大 C。

最后，有别于 logistic 回归的一点是 SVM 算法的假设函数并不代表 y = 0 或 1 的概率，而只是输出 0 或 1：
$$
h_{\theta}(x)= \begin{cases}
1 \quad if\: \theta^Tx\geq0 \\
0 \quad otherwise
\end{cases}
$$
​           

### 12.2. Large Margin Intuition

把 SVM 算法当作**大间距分类器（Large Margin Classifier）**来理解是比较直观的理解方式。

下图回顾了 SVM 算法的最小化代价函数的结论：

![](https://photo.hushhw.cn/20190906144938.png)

如果代价函数中的 C 被设置的非常非常大，想要是的代价函数最小，就必须让代价函数项为 0 ，即：
$$
\begin{align}
J(\theta) &= C\cdot0 + \frac12\sum^n_{j=1}\theta^2_j \\
when: \quad&\theta^Tx^{(i)}\geq 1 \quad if \:y^{(i)}=1 \\
&\theta^Tx^{(i)}\leq -1 \quad if \:y^{(i)}=0 \\
\end{align}
$$
![](https://photo.hushhw.cn/20190906150428.png)

在上图的决策边界选择中，黑线决策边界的选择是符合 SVM 算法要求的，黑线有与正负数据集更大的距离，这个距离叫做支持向量机的**间距（margin）**，因此支持向量机有时被称为**大间距分类器（Large Margin Classifier）**。在 SVM 算法中，决策边界的性质是尽可能地远离正数据集与负数据集。

事实上，支持向量机 SVM 现在比大间距分类器要成熟得多，使用大间距分类器的学习算法容易受到异常点（outlier）的影响，如下图中会因为一个异常点导致决策边界发生很大的偏移。

![](https://photo.hushhw.cn/20190906151419.png)

如果想要使得这些异常点不过分影响决策边界，这时应该减小 C 的数值。

