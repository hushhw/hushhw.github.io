---
title: 吴恩达《机器学习》笔记（六）——神经网络的表述
comments: true
mathjax: true
toc: true
tocnumber: false
music: false
image: false
tags:
  - Machine Learning
categories:
  - 笔记整理
  - 机器学习
description: 本节表述了神经网络在机器学习中的应用及表达形式，并做了简单的举例说明。
abbrlink: e75b0cdb
date: 2019-08-11 14:53:52
---

> 本节表述了神经网络在机器学习中的应用及表达形式，并做了简单的举例说明。



## Neural Networks: Representation

### 8.1. Non-linear Hypotheses

我们前面学到的线性回归和逻辑回归在面临有很多特征时，计算的负荷都会非常大。

使用非线性的多项式项能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于 100 个变量，我们希望用这 100 个特征来构建一个非线性的多项式模型，结果将是数据非常惊人的特征组合，即便我们只采用两两特征的组合（$x_1x_2 + x_1x_3 + x_1x_4 + \cdots + x_2x_3 + x_2x_4 + \cdots + x_{99}x_{100}$），这样也有接近 5000 个组合而成的特征，这对于一般的逻辑回归来说需要计算的特征太多了。

所以，我们需要引入神经网络。

​             

### 8.2. Neurons and the Brain

神经网络是计算量有些偏大的算法，大概由于近些年计算机的运行速度变快，才足以真正运行起大
规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。

​                

### 8.3. Model Representation

#### 8.3.1. 简单表达

为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？

每一个神经元都可以被认为是一个**处理单元/神经核（processing unit/Nucleus）**，它含有许多**输入/树突（input/Dendrite）**，并且有一个**输出/轴突（output/Axon）**。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 

![](https://photo.hushhw.cn/20190811162118.png)

神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元采纳一些特征作为输出，并且根据本身的模型提供一个输出。下面是一个以 logistic 回归模型作为自身学习模型的神经元示例：

![](https://photo.hushhw.cn/20190904173829.png)

其中，在神经网络中，参数（parameter）又可被称为**权重（weight）**；$x_0=1​$ 称为**偏置单元（bias unit）**，是否写出根据情况来定；红色的圈代表神经元，又称**激活单元（activation unit）**；g(z) 称为**激活函数（activation function）**，一般指「由……驱动的神经元计算函数 g(z)」，上面就是逻辑激活函数。

我们推演到多个神经元的情况：

![](https://photo.hushhw.cn/20190904175859.png)

其中，第一层是输入层（Input layer），第三层是输出层（Output Layer），而中间数据处理的一层为隐藏层（Hidden Layers），有输入层加权组合后重新映射而成，每一层都可以加上偏置单元。

其中一些符号标记：
$$
\begin{align}
a_i^{(j)} &= 第j层的第i个激活单元 \\
\Theta^{(j)} &= 控制从第j层到j+1层的映射函数的权重矩阵
\end{align}
$$
其中，每一个激活结点都可以用 sigmoid 激活函数表示：

![](https://photo.hushhw.cn/20190904191827.png)

由矩阵乘法可知：因此如果要从含有 $s_j$ 个单元的第 j 层映射到含有 $s_j+1$ 个单元的第 j+1 层，那么权重矩阵 Θ(j) 的尺寸为 $s_{j+1}×(s_j+1)​$，其中的 +1是因为要考虑偏置单元。

这种从左到右的算法称为**前向传播算法（Forward propagation）**。

#### 8.3.2. 向量形式表达

下面利用向量形式来表达该算法，以上面神经网络为例，试着计算第二层的值：
$$
x=\left[ \begin{matrix} x_0 \\x_1 \\ x_2 \\x_3 \end{matrix} \right]、 
z^{(2)} = \left[ \begin{matrix} z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\end{matrix} \right]
$$
其中，$z^{(2)} = \Theta^{(1)}x，a^{(2)} = g(z^{(2)})$。

进而，我们得到了参数 z 计算的表达式，计算第 j-1 层到第 j 层映射 g(z) 的参数 z：
$$
z^{(j)} = \Theta^{(j-1)}a^{(j-1)}
$$
接着，用同样的方法计算下一层的信息：
$$
z^{(j+1)} = \Theta^{(j)}a^{(j)}
$$
这样上例中 $h_{\theta}(x)=a^{(3)}=g(z^{(3)})$。

#### 8.3.3. 例子

从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 

神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。 

![](https://photo.hushhw.cn/20190905075722.png)

![](https://photo.hushhw.cn/20190905075911.png)

当输入特征为布尔值时，我们可以用一个单一的激活层作为二元逻辑运算符（binary logical operators），为了表示不同的运算符，只需要修改不同的权重即可。

​          

### 8.7. Multicalss Classification

当我们有不止两种分类时，可通过神经网络算法来输出不同的结果进行判断，如第一个值是 1 或 0 来预测是否是行人，第二个值是否是 1 或 0 来预测是否是汽车等。

![](https://photo.hushhw.cn/20190905171102.png)

​            

## 参考

> [驿舟小站](https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-6/)
>
> [Coursera-ML-AndrewNg-Notes](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)
>
> [斯坦福大学 2014 机器学习](https://www.coursera.org/course/ml)



