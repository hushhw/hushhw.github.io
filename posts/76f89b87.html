<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="hushhw × Wiki"><meta name="google-site-verification"><meta name="baidu-site-verification"><title>吴恩达《机器学习》笔记（五）——正则化 | hushhw × Wiki</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-132675789-2','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '972cc0516975a00c2c5900eb5e98039d';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();

</script><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.css" rel="stylesheet"><script>(function(){ var bp = document.createElement('script'); var curProtocol = window.location.protocol.split(':')[0];if (curProtocol === 'https') {bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';} else {bp.src = 'http://push.zhanzhang.baidu.com/push.js';}var s = document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp, s);})();</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-132675789-2"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-132675789-2');
</script><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">吴恩达《机器学习》笔记（五）——正则化</h1><a id="logo" href="/.">hushhw × Wiki</a><p class="description"></p></div><div id="nav-menu"><a href="/tech/"><i class="fa fa-battery-full"> 技术</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"> <h1 class="post-title">吴恩达《机器学习》笔记（五）——正则化</h1><div class="post-meta">2019-08-01<script src="https://photo.hushhw.cn/busuanzi.pure.mini.js" async></script><span class="busuanzi_container_page_pv"> • <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> • </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1,282</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> • <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 4</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/posts/76f89b87.html#vcomment"><span class="valine-comment-count" data-xid="/posts/76f89b87.html"></span><span> 条评论</span></a><div class="post-content"><blockquote>
<p>评价一个模型拟合度是否优良的参考之一是它与实际数据集的偏差程度，我们用代价函数来定量，一般代价函数越小越好，但是当将它们应用在某些特定的机器学习应用时，会遇到<strong>过拟合（over-fitting）</strong>的问题。这时就需要采取一些措施，正则化就是其中一种方式。</p>
</blockquote>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="7-1-Overfitting"><a href="#7-1-Overfitting" class="headerlink" title="7.1. Overfitting"></a>7.1. Overfitting</h3><p>机器学习训练的目的是为了让模型更好的拟合实际情况，从而指导我们进行预测。评价一个模型拟合度是否优良的参考之一是它与实际数据集的偏差程度，我们用代价函数来定量，一般代价函数越小越好，但是当将它们应用在某些特定的机器学习应用时，会遇到<strong>过拟合（over-fitting）</strong>的问题。</p>
<p>下图是一个回归问题的例子：</p>
<p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。 </p>
<p><img src="https://photo.hushhw.cn/20190805144528.png" alt></p>
<p>下图是一个分类问题的例子：</p>
<p>以多项式理解，𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 </p>
<p><img src="https://photo.hushhw.cn/20190805144934.png" alt></p>
<p>由上述分析，我们可以由两种解决过拟合的方案：</p>
<ul>
<li>丢弃一些不能帮助我们正确预测的特征：可以手动选择保留哪些特征，或者使用一些模型选择的算法来帮忙</li>
<li>正则化：保留所有的特征变量，但是减少量级或参数 $\theta_j$ 的大小</li>
</ul>
<p>​            </p>
<h3 id="7-2-Cost-Function"><a href="#7-2-Cost-Function" class="headerlink" title="7.2. Cost Function"></a>7.2. Cost Function</h3><p>为了避免过拟合的问题，这里介绍前面提到的正则化的方法。</p>
<p>上面的回归问题中我们知道，正是那些高次项导致了过拟合的问题，所以如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。</p>
<p>所以，这里我们通过降低三次项和四次项的权重，即令 $𝜃_3$ 和 $𝜃_4$ 的值很小，使得拟合模型在大方向上是「二次多项式」在发挥作用。我们要做的便是修改代价函数，在其中 $𝜃_3$ 和 $𝜃_4$ 设置一点惩罚（penalize）。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 $𝜃_3$ 和 $𝜃_4$ 。 </p>
<p><img src="https://photo.hushhw.cn/20190805152238.png" alt></p>
<p>通过这样的代价函数选择出的 $𝜃_3$ 和 $𝜃_4$ 对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]</script><p>其中 𝜆 又称为<strong>正则化参数（Regularization Parameter）</strong>。 根据惯例，我们不对 $𝜃_0​$ 进行惩罚。</p>
<p>对 𝜆 的取值要合理，如果 𝜆 过大的话，为了使得代价函数尽可能小，所有的 𝜃 值（不包括 $𝜃_0$）就都会在一定程度上减小，我们只能得到一条平行于 𝑥 轴的直线，这样会导致<strong>欠拟合（underfitting)</strong>。</p>
<p>​                 </p>
<h3 id="7-3-Regularized-Linear-Regression"><a href="#7-3-Regularized-Linear-Regression" class="headerlink" title="7.3. Regularized Linear Regression"></a>7.3. Regularized Linear Regression</h3><p>对于线性回归的求解，我们之前推到了两种学习算法：</p>
<ul>
<li>梯度下降</li>
<li>正规方程</li>
</ul>
<p>正则化线性回归的代价函数为：</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m}\left[ \sum^{m}_{i=1}(h_{\theta}(x^{(i)}-y^{(i)})^2 + \lambda\sum^n_{j=1}\theta^2_j\right]</script><p>如果我们要使用梯度下降法令这个代价函数最小化，其正则化迭代式如下：</p>
<script type="math/tex; mode=display">
\begin{align}
Repeat \{ \\
θ_0 &:= θ_0-\alpha\frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_0 \\
θ_j &:= θ_j-\alpha \left[ \left( \frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}θ_j \right] \ j\in\{1,2,\cdots,n\}\\
\}
\end{align}</script><p>将 $θ_0$ 单独分开的原因上面提到了，其对应的特征量是 1，约定俗成不必正则化。上式可在整理为：</p>
<script type="math/tex; mode=display">
θ_j := θ_j(1-\alpha\frac{\lambda}{m})-\alpha\frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j</script><p>在这个形式下，我们可以理解得到，每一次迭代都会多乘一个小于一的系数 $1–α\fracλm$，进而缩小。</p>
<p>​              </p>
<p>如果我们要使用正规方程来求解正则化线性回归模型，方法如下：</p>
<p><img src="https://photo.hushhw.cn/20190805165041.png" alt></p>
<script type="math/tex; mode=display">
\theta = (X^TX + \lambda L)^{-1}X^Ty  \\
where \ L = \left[
\begin{matrix}
0 \qquad\qquad\qquad\qquad \\
\qquad 1  \qquad\qquad\qquad\\
\qquad\qquad1 \qquad\qquad \\
\qquad\qquad\qquad\ddots \qquad \\
\qquad\qquad\qquad\qquad 1
\end{matrix}
\right]</script><pre><code> 其中 L 为(n+1)*(n+1)。
</code></pre><p>​              </p>
<h3 id="7-4-Regularized-Logistic-Regression"><a href="#7-4-Regularized-Logistic-Regression" class="headerlink" title="7.4. Regularized Logistic Regression"></a>7.4. Regularized Logistic Regression</h3><p><img src="https://photo.hushhw.cn/20190811143857.png" alt></p>
<p>对于 logistic 回归模型，如果你得模型中有很多的特征，有无关紧要的多项式，这些大量的特征就会造成过拟合现象。</p>
<p>logistic 回归模型正则化后的代价函数为：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}\sum^{m}_{i=1}\left[y^{(i)}log\ h_{\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right] + \frac{λ}{2m}\sum^n_{j=1}\theta^2_j</script><p>如果使用梯度下降算法最小化代价函数的话，其正则化迭代式为：</p>
<script type="math/tex; mode=display">
\begin{align}
Repeat \{ \\
θ_0 &:= θ_0-\alpha\frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_0 \\
θ_j &:= θ_j-\alpha \left[ \left( \frac1m\sum^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x^{(i)}_j\right) + \frac{\lambda}{m}θ_j \right] \ j\in\{1,2,\cdots,n\}\\
\}
\end{align}</script><p>看上去和线性回归一样，但是因为 $h_{\theta}(x) = g({\theta}^TX)$，所以与线性回归不同。</p>
<p>​                 </p>
<h2 id="专业名词整理"><a href="#专业名词整理" class="headerlink" title="专业名词整理"></a>专业名词整理</h2><ul>
<li><code>overfitting</code>：过拟合、<code>underfitting</code>：欠拟合</li>
</ul>
<p>​            </p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<p><a href="https://www.zhouyongyi.com/andrew-ng-machine-learning-notes-5/" target="_blank" rel="noopener">驿舟小站</a></p>
<p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">Coursera-ML-AndrewNg-Notes</a></p>
<p><a href="https://www.coursera.org/course/ml" target="_blank" rel="noopener">斯坦福大学 2014 机器学习</a></p>
</blockquote>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>hushhw</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/posts/76f89b87.html">吴恩达《机器学习》笔记（五）——正则化</a></li><li class="post-copyright-date"><strong>发布时间：</strong>2019年08月01日 - 16:54:09</li><li class="post-copyright-updated"><strong>更新时间：</strong>2021年02月03日 - 6:56:56</li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://wiki.hushhw.cn/posts/76f89b87.html" data-id="ckkp2xthc000mf8tl96b1bx8c" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAABx0lEQVR42u3ay46DMAwF0P7/T3ek2Q6FazukjHRYVWoIBxaWH3m94uv9e/39fb7mfOWnfxdcuLi4Y+779EqIn6Dn+yR3HTwXFxd3IzcPTMnL5MTcgIuL+0xuNQPJEyBcXNz/y83Lm15ihIuL+zRuksRM0po8tC2r1XBxcQfcScN01e9N/V1cXNzxVCIJar3Q1nw6Li7uFm7ymLxoyYNUL4XCxcXdz60GrPOSppq+5Pvj4uLu51YD2aRASj7ZwV24uLg3c6sty/kAdRQocXFxv8RdlYisGpwcdHFwcXFv5lZDT280u6Dg6fVmcHFxW9xJKySnJKOUpF2Li4u7k9sbckzGsc2AiIuLu5GbB6l8Ta9pctF+xcXF3cjNk5u8gKkez4rouLi4W7iTmqJ6FKPaJL1omOLi4t7GzROa/HBVdTdcXNxncnuBKUmMktKo8AlwcXE3cnuJSHVk22unFnqruLi4i7jVgDI5kHFHuwQXF/c+bmHMWTzIle9TSLZwcXE3cqtjkqQ06hU/F2twcXEfxq1Cq4lO9PK4uLgP5lbToLysuii0cHFxN3KT4qdXrvSaIwvaIri4uAPuvGFaPgsWD2BwcXG3c38AtMjdRBEiz4IAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a></div><div class="post-nav"><a class="pre" href="/posts/e75b0cdb.html">吴恩达《机器学习》笔记（六）——神经网络的表述</a><a class="next" href="/posts/4884afb4.html">Google Hacking 语法使用总结</a></div><div class="recommended_posts"><h2>相关文章：</h2><li><a href="https://wiki.hushhw.cn/posts/18a3eba2.html" target="_blank">吴恩达《机器学习》笔记（七）——神经网络的学习</a></li><li><a href="https://wiki.hushhw.cn/posts/e75b0cdb.html" target="_blank">吴恩达《机器学习》笔记（六）——神经网络的表述</a></li><li><a href="https://wiki.hushhw.cn/posts/4884afb4.html" target="_blank">Google Hacking 语法使用总结</a></li><li><a href="https://wiki.hushhw.cn/posts/f72c23ef.html" target="_blank">吴恩达《机器学习》笔记（四）——Logistic回归</a></li></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//photo.hushhw.cn/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'YVxvrCNxpTLOcs83UfKCUOaI-gzGzoHsz',
  appKey:'5eaXH2FJm2nxSBIQL4INFBbW',
  placeholder:'居然什么也不说，哼',
  avatar:'robohash',
  guest_info:guest_info,
  pageSize:'10'
})</script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-blind"> Contents </i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization"><span class="toc-number">1.</span> <span class="toc-text">Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Overfitting"><span class="toc-number">1.1.</span> <span class="toc-text">7.1. Overfitting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Cost-Function"><span class="toc-number">1.2.</span> <span class="toc-text">7.2. Cost Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Regularized-Linear-Regression"><span class="toc-number">1.3.</span> <span class="toc-text">7.3. Regularized Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-Regularized-Logistic-Regression"><span class="toc-number">1.4.</span> <span class="toc-text">7.4. Regularized Logistic Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#专业名词整理"><span class="toc-number">2.</span> <span class="toc-text">专业名词整理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">3.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© 2018 - 2021 | <span class="footicon"><i class="fa fa-meh-o"></i></span><a rel="nofollow" target="_blank" href="https://wiki.hushhw.cn">hushhw × Wiki</a> | <a rel="nofollow" target="_blank" href="http://www.miibeian.gov.cn/">鄂ICP备17007175号-1</a><br> 
Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo | </a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho | </a>Accelerated by <a rel="nofollow" target="_blank" href="https://www.upyun.com/index.html"><img class="upyunimg" src="https://photo.hushhw.cn/images/又拍云120180930001119303.png"></a><br> <script src="https://photo.hushhw.cn/busuanzi.pure.mini.js" async="async"></script><span id="busuanzi_container_site_uv"></span><i class="fa fa-user-md"></i><span> 访问人数<span id="busuanzi_value_site_uv"></span></span> | <span id="busuanzi_container_site_pv"> </span><span class="footicon"><i class="fa fa-eye"></i></span><span> 访问量<span id="busuanzi_value_site_pv"></span></span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script><script type="text/javascript" src="/js/toc.js?v=0.0.0" async></script></div></body></html>